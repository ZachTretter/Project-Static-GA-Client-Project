{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSI BOS 11 (May 2020) Project 5\n",
    "\n",
    "Alex Golden, Jungmoon Ham, Luke Podsiadlo, Zach Tretter\n",
    "\n",
    "Workbook 3 - Speech to Text Transcription\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T19:47:09.907464Z",
     "start_time": "2020-05-13T19:47:09.903512Z"
    }
   },
   "source": [
    "## Speech Recognition\n",
    "\n",
    "#### Workflow Steps\n",
    "\n",
    "1. Import audio segments (.wav files)\n",
    "\n",
    "2. Transcribe via google's cloud speech-to-text API\n",
    "\n",
    "3. Export results as dataframe\n",
    "\n",
    "Core code adapted from\n",
    "* DSI-SF-9 [(Grant Wilson, J. Hall, Gabriel Perez Prieto)](https://github.com/GWilson97/san_francisco_dispatch_audio_mapping/blob/master/code/03a_speech_to_text.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T15:47:56.395514Z",
     "start_time": "2020-05-13T15:47:56.391509Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade google-cloud-speech\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "from google.cloud.speech_v1p1beta1 import enums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Credentials for Google Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T15:47:56.863670Z",
     "start_time": "2020-05-13T15:47:56.854691Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_key = \"<><><><><><><><><><>\"\n",
    "\n",
    "key_name = '<><><><><><><><><><>'\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = path_to_key + key_name\n",
    "\n",
    "client = speech.SpeechClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Input Path (Specific to Your Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T15:48:01.786450Z",
     "start_time": "2020-05-13T15:48:01.767495Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_path = \"./wav_output/\"\n",
    "\n",
    "os.listdir(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T00:24:36.025031Z",
     "start_time": "2020-05-12T00:24:36.023029Z"
    }
   },
   "source": [
    "### Establish Output Path (Specific to Your Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T19:42:11.952552Z",
     "start_time": "2020-05-13T19:42:11.945547Z"
    }
   },
   "outputs": [],
   "source": [
    "output_path = \"./transcribe_output/\"\n",
    "\n",
    "file_name = 'Feed25818_May2020_10AM_to_12AM_transcript.csv'\n",
    "\n",
    "os.listdir(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T00:26:34.835965Z",
     "start_time": "2020-05-12T00:26:34.832957Z"
    }
   },
   "source": [
    "### Transcribe to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T18:05:13.188779Z",
     "start_time": "2020-05-13T15:51:21.784478Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for sample_audio in os.listdir(input_path):\n",
    "    loop_time = time.time()\n",
    "    \n",
    "    # Examine wav files\n",
    "    if sample_audio.endswith('.wav'):\n",
    "        \n",
    "        # Open files\n",
    "        with io.open(input_path + sample_audio,'rb') as audio_to_transcribe:\n",
    "            content = audio_to_transcribe.read()\n",
    "            audio = speech.types.RecognitionAudio(content = content)\n",
    "            \n",
    "        # Declare speech recognition parameters\n",
    "        config = speech.types.RecognitionConfig(\n",
    "            encoding = enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "            sample_rate_hertz = 22050,\n",
    "            language_code = 'en-US',\n",
    "            audio_channel_count = 1,\n",
    "            enable_separate_recognition_per_channel = True,\n",
    "            use_enhanced = True,\n",
    "            model = 'phone_call',\n",
    "            speech_contexts = [{'boost':20.0}]\n",
    "        )\n",
    "        \n",
    "        # This models equivalent of fit/predict\n",
    "        response = client.recognize(config,audio)\n",
    "        \n",
    "        # Build Dictionary that becomes a Dataframe\n",
    "        for result in response.results:\n",
    "            d = {}\n",
    "            d['transcript'] = result.alternatives[0].transcript\n",
    "            d['confidence'] = result.alternatives[0].confidence\n",
    "            d['file_name'] = sample_audio\n",
    "            d['audio_length'] = round(int(sample_audio.split(\"-\")[-1].split(\".\")[0])/1_000,1)\n",
    "            processing_time = round(time.time() - loop_time,1)\n",
    "            d['transcribe_time'] = processing_time\n",
    "            df = df.append(d, ignore_index=True)\n",
    "                    \n",
    "        print(f\"File {sample_audio} transcribed to df in {round(processing_time,0)} secs\")\n",
    "\n",
    "print(f'total time of {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T18:05:30.341337Z",
     "start_time": "2020-05-13T18:05:30.302472Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)  \n",
    "\n",
    "df = df[['file_name',\n",
    "         'audio_length',\n",
    "         'transcribe_time',\n",
    "         'confidence',\n",
    "         'transcript']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T01:00:37.348178Z",
     "start_time": "2020-05-12T01:00:37.344188Z"
    }
   },
   "source": [
    "### Export CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T18:05:44.177905Z",
     "start_time": "2020-05-13T18:05:44.151938Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(output_path + file_name,\n",
    "          index_label = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
