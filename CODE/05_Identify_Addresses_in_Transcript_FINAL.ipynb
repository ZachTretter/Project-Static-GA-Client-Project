{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:50:17.936419Z",
     "start_time": "2020-05-17T15:50:17.931426Z"
    }
   },
   "source": [
    "DSI BOS 11 (May 2020) Project 5\n",
    "\n",
    "Alex Golden, Jungmoon Ham, Luke Podsiadlo, Zach Tretter\n",
    "\n",
    "Workbook 5 - Natural Language Processing\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Addresses in Transcripts\n",
    "\n",
    "This workbook leverages the work of [DSI-SF-9 (Grant Wilson, J. Hall, Gabriel Perez Prieto)](https://github.com/GWilson97/san_francisco_dispatch_audio_mapping/blob/master/code/04_get_street_name.ipynb)\n",
    "\n",
    "### Workflow Steps\n",
    "\n",
    "1. [Imports](#Imports)\n",
    "2. [Read in Transcripts](#Read-in-Transcripts)\n",
    "3. [Import List of Streets](#Import-List-of-Streets)\n",
    "4. [Tokenize the Transcribed Audio](#Tokenize-the-Transcribed-Audio)\n",
    "5. [Identify and Match Street Names in Transcripts](#Identify-and-Match-Street-Names-in-Transcripts)\n",
    "6. [Find Possible Street Numbers](#Find-Possible-Street-Numbers)\n",
    "7. [Add Street Numbers to Dataframe](#Add-Street-Numbers-to-Dataframe)\n",
    "8. [Generate Potential Addresses](#Generate-Potential-Addresses)\n",
    "9. [Drop Blanks and Export](#Drop-Blanks-and-Export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:27:34.483588Z",
     "start_time": "2020-05-17T16:27:33.764772Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ! pip install usaddress\n",
    "# ! pip install spacy\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "from spacy.attrs import LOWER \n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "import numpy as np\n",
    "import usaddress\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:19:55.491662Z",
     "start_time": "2020-05-17T16:19:55.489636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set column width to be larger to display more content\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Transcripts\n",
    "\n",
    "* feed_25818_raw_transcript.csv\n",
    "* feed_25818_enhanced_transcript.csv\n",
    "* watertown_manhunt_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:19:55.505623Z",
     "start_time": "2020-05-17T16:19:55.492627Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../DATASETS/transcripts/feed_25818_raw_transcript.csv\")\n",
    "df1['dolby'] = False\n",
    "df1 = df1[['file_name','confidence','dolby','transcript']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:19:55.904902Z",
     "start_time": "2020-05-17T16:19:55.892926Z"
    }
   },
   "outputs": [],
   "source": [
    "df_enhanced = pd.read_csv(\"../DATASETS/transcripts/feed_25818_enhanced_transcript.csv\")\n",
    "df_enhanced['dolby'] = True\n",
    "df_enhanced = df_enhanced[['file_name','confidence','dolby','transcript']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:19:56.254227Z",
     "start_time": "2020-05-17T16:19:56.237229Z"
    }
   },
   "outputs": [],
   "source": [
    "df_watertown = pd.read_excel(\"../DATASETS/transcripts/watertown_manhunt_transcript.xlsx\")\n",
    "df_watertown = df_watertown.drop(columns = ['Unnamed: 1'])\n",
    "df_watertown['confidence'] = 'manual'\n",
    "df_watertown['dolby'] = False\n",
    "df_watertown['file_name'] = 'watertown_manhunt'\n",
    "df_watertown = df_watertown[['file_name','confidence','dolby','transcript']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:52:24.747232Z",
     "start_time": "2020-05-17T16:52:24.725797Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1,\n",
    "                df_enhanced,\n",
    "                df_watertown]).reset_index().drop(columns=['index'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import List of Streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:20:00.630118Z",
     "start_time": "2020-05-17T16:20:00.623155Z"
    }
   },
   "outputs": [],
   "source": [
    "street_list = pd.read_csv('../DATASETS/ancillary_csv/Metro_West_Streets.csv')\n",
    "streets_list = street_list['0'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Transcribed Audio\n",
    "\n",
    "Adapted from [Michael Allen (pythonhealthcare.org)](https://pythonhealthcare.org/2018/12/14/101-pre-processing-data-tokenization-stemming-and-removal-of-stop-words/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:20:01.731382Z",
     "start_time": "2020-05-17T16:20:01.269501Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    tran = row['transcript']\n",
    "    tokens = nltk.word_tokenize(tran)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "df['tokens'] = df.apply(identify_tokens,\n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and Match Street Names in Transcripts\n",
    "\n",
    "Adapted from [DSI-SF-9 (Grant Wilson, J. Hall, Gabriel Perez Prieto)](https://github.com/GWilson97/san_francisco_dispatch_audio_mapping/blob/master/code/04_get_street_name.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:20:43.491616Z",
     "start_time": "2020-05-17T16:20:02.357039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the Spacy Matcher Function\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a matching function\n",
    "def on_match(matcher, doc, id, matches):\n",
    "    return matches\n",
    "\n",
    "# building patterns for every road name\n",
    "def build_pattern(road_name):\n",
    "    list_words = road_name.split(' ')\n",
    "    # ensure capitlization does not affect the model \n",
    "    pattern = [{'LOWER': word.lower()} for word in list_words]\n",
    "    return pattern\n",
    "\n",
    "# Get a pattern of every road\n",
    "for road in streets_list:\n",
    "    matcher.add(road, on_match, build_pattern(road))\n",
    "    \n",
    "# capitalize all the strings\n",
    "def capitalize_string(string_in):\n",
    "    words = string_in.split(' ')\n",
    "    string_out = ''\n",
    "    for i in words:\n",
    "        string_out += i.capitalize() + ' '\n",
    "    string_out = string_out[:-1]\n",
    "    return string_out   \n",
    "    \n",
    "# Look for locations in the transcript, then extract them\n",
    "def location_extraction_context(string_in):\n",
    "    doc = nlp(string_in)\n",
    "    string_out = ''\n",
    "    list_words = string_in.split(' ')\n",
    "    matches = matcher(doc)\n",
    "    if len(matches) == 0:\n",
    "        return None\n",
    "\n",
    "    # loop through the matches and make sure they all follow the same format\n",
    "    for match in matches:\n",
    "        list_pattern = matcher.get(match[0])[1][0]\n",
    "        for token in list_pattern:\n",
    "            string_out += token['LOWER'] + ' '\n",
    "        string_out += ', '\n",
    "    string_out = string_out[:-3]\n",
    "    string_out = capitalize_string(string_out)\n",
    "    return string_out\n",
    "\n",
    "# Add a column consisting of the extracted streets\n",
    "df['streets'] = df['transcript'].map(location_extraction_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Possible Street Numbers\n",
    "\n",
    "Adapted from [DSI-SF-9 (Grant Wilson, J. Hall, Gabriel Perez Prieto)](https://github.com/GWilson97/san_francisco_dispatch_audio_mapping/blob/master/code/04_get_street_name.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:20:57.450548Z",
     "start_time": "2020-05-17T16:20:56.398035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creat list to house data from all addresses\n",
    "addresses = []\n",
    "\n",
    "# Loop Through all DataFrame's rows\n",
    "for row in df['transcript']:\n",
    "    # Create dictionary to house data for each row of the DataFrame\n",
    "    d = {}\n",
    "    \n",
    "    # Parse through rows and house results in a list\n",
    "    list_tuples = usaddress.parse(row)\n",
    "    \n",
    "    # Create variable to house list of possible numbers\n",
    "    numbers = []\n",
    "    \n",
    "    # Loop through each value in the list created\n",
    "    for i, n in enumerate(list_tuples):\n",
    "        \n",
    "        # Get addresses' numbers\n",
    "        if list_tuples[i][1] == 'AddressNumber':\n",
    "            \n",
    "            # Append numbers to list\n",
    "            numbers.append(n[0])\n",
    "    \n",
    "    # Include keys and values into d\n",
    "    d['numbers'] = numbers\n",
    "    \n",
    "    # Append d to addresses\n",
    "    addresses.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Street Numbers to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:20:58.159876Z",
     "start_time": "2020-05-17T16:20:58.147906Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, \n",
    "                pd.DataFrame(addresses)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:20:58.714298Z",
     "start_time": "2020-05-17T16:20:58.703327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop NaNs\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True,\n",
    "               inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:21:01.017174Z",
     "start_time": "2020-05-17T16:21:01.012223Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Potential Addresses\n",
    "\n",
    "Adapted from [DSI-SF-9 (Grant Wilson, J. Hall, Gabriel Perez Prieto)](https://github.com/GWilson97/san_francisco_dispatch_audio_mapping/blob/master/code/04_get_street_name.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:21:02.837883Z",
     "start_time": "2020-05-17T16:21:02.341594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creat list to house data for possible addresses\n",
    "possibilities = []\n",
    "\n",
    "# Loop Through all DataFrame's rows\n",
    "for i in range(0, df.shape[0]):\n",
    "    \n",
    "    # Create variables to temporarily house information\n",
    "    final_poss = []\n",
    "    d = {}\n",
    "    number_poss = []\n",
    "    \n",
    "    # Loop through values in each row / numbers\n",
    "    for row in df[i:i+1]['numbers']:\n",
    "        for a_number in row:\n",
    "            number_poss.append(a_number)\n",
    "    \n",
    "    # Loop through values in each row / streets\n",
    "    street_poss = []\n",
    "    for row2 in [x.split(',') for x in df[i:(i+1)]['streets']][0]:\n",
    "        for j in row2.split(','):\n",
    "            street_poss.append(j.strip())\n",
    "\n",
    "    # Concatenate numbers and streets\n",
    "    for i in number_poss:\n",
    "        for j in street_poss:\n",
    "            final_poss.append(i + ' ' + j)\n",
    "\n",
    "    # Append all possibilities to list\n",
    "    d['full_streets'] = list(set(final_poss))\n",
    "    possibilities.append(d)\n",
    "\n",
    "# Concatenate dataframes\n",
    "df = pd.concat([df, pd.DataFrame(possibilities)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Blanks and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:21:07.124134Z",
     "start_time": "2020-05-17T16:21:07.111192Z"
    }
   },
   "outputs": [],
   "source": [
    "df['full_streets'] = df['full_streets'].map(lambda x: np.nan if len(x) == 0 else x)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:21:07.506263Z",
     "start_time": "2020-05-17T16:21:07.501307Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:21:09.403209Z",
     "start_time": "2020-05-17T16:21:09.381290Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:22:06.934469Z",
     "start_time": "2020-05-17T16:22:06.917754Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"../DATASETS/dataframe_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize \"Confidence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:27:46.753517Z",
     "start_time": "2020-05-17T16:27:46.748561Z"
    }
   },
   "outputs": [],
   "source": [
    "df_confid = df[(df['file_name']!='watertown_manhunt')\n",
    "   &(df['file_name'].str.contains('enhanced')!=True)]['confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:27:47.122605Z",
     "start_time": "2020-05-17T16:27:47.117617Z"
    }
   },
   "outputs": [],
   "source": [
    "df_confid.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:27:48.280301Z",
     "start_time": "2020-05-17T16:27:48.141647Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_confid.plot(\n",
    "    kind = 'hist',\n",
    "    x = 'Confidence',\n",
    "    title = 'Transcription Confidence',\n",
    "    figsize = (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T16:27:49.633813Z",
     "start_time": "2020-05-17T16:27:49.492192Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(df_confid, kde = False)\n",
    "ax.set_title('Transcription Confidence')\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlabel(\"Confidence\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
