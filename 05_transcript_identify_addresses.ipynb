{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import All Neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:33:36.386790Z",
     "start_time": "2020-05-14T01:33:36.372815Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ! pip install usaddress\n",
    "# ! pip install spacy\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "from spacy.attrs import LOWER \n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "import numpy as np\n",
    "import usaddress\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:25:43.713392Z",
     "start_time": "2020-05-14T01:25:43.709370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set column width to be larger to display more content\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Transcribed Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:25:44.666900Z",
     "start_time": "2020-05-14T01:25:44.652938Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./Datasets/transcribed_audio/Feed25818_May2020_01AM_to_03PM_transcript_Alex.csv\")\n",
    "df1 = df1[['file_name','confidence','transcript']]\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:25:45.096293Z",
     "start_time": "2020-05-14T01:25:45.077347Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"./Datasets/transcribed_audio/Feed25818_May2020_10AM_to_12AM_transcript_Zach.csv\")\n",
    "df2 = df2.drop(columns = ['audio_length', 'transcribe_time'])\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:25:45.602993Z",
     "start_time": "2020-05-14T01:25:45.581053Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_excel(\"./Datasets/transcribed_audio/watertown_manhunt_transcript.xlsx\")\n",
    "df3 = df3.drop(columns = ['Unnamed: 1'])\n",
    "df3['confidence'] = None\n",
    "df3['file_name'] = 'watertown_manhunt'\n",
    "df3 = df3[['file_name','confidence','transcript']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:30:17.028808Z",
     "start_time": "2020-05-14T01:30:17.012838Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df3,df2]).reset_index().drop(columns=['index'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import List of Streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:29:28.971786Z",
     "start_time": "2020-05-14T01:29:28.958825Z"
    }
   },
   "outputs": [],
   "source": [
    "street_list = pd.read_csv('./Datasets/Metro_West_Streets.csv')\n",
    "streets_list = street_list['0'].tolist()\n",
    "streets_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Transcribed Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:28:20.655103Z",
     "start_time": "2020-05-14T01:28:20.301434Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    tran = row['transcript']\n",
    "    tokens = nltk.word_tokenize(tran)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "df['tokens'] = df.apply(identify_tokens,\n",
    "                        axis=1)\n",
    "\n",
    "# adpapted from Michael Allen (pythonhealthcare.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:30:04.747101Z",
     "start_time": "2020-05-14T01:30:04.733894Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and Match Street Names from the Audio Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:32:10.430983Z",
     "start_time": "2020-05-14T01:32:10.413051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the Spacy Matcher Function\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a matching function\n",
    "def on_match(matcher, doc, id, matches):\n",
    "    return matches\n",
    "\n",
    "# building patterns for every road name\n",
    "def build_pattern(road_name):\n",
    "    list_words = road_name.split(' ')\n",
    "    # ensure capitlization does not affect the model \n",
    "    pattern = [{'LOWER': word.lower()} for word in list_words]\n",
    "    return pattern\n",
    "\n",
    "# Get a pattern of every road\n",
    "for road in streets_list:\n",
    "    matcher.add(road, on_match, build_pattern(road))\n",
    "    \n",
    "# capitalize all the strings\n",
    "def capitalize_string(string_in):\n",
    "    words = string_in.split(' ')\n",
    "    string_out = ''\n",
    "    for i in words:\n",
    "        string_out += i.capitalize() + ' '\n",
    "    string_out = string_out[:-1]\n",
    "    return string_out   \n",
    "    \n",
    "# Look for locations in the transcript, then extract them\n",
    "def location_extraction_context(string_in):\n",
    "    doc = nlp(string_in)\n",
    "    string_out = ''\n",
    "    list_words = string_in.split(' ')\n",
    "    matches = matcher(doc)\n",
    "    if len(matches) == 0:\n",
    "        return None\n",
    "\n",
    "    # loop through the matches and make sure they all follow the same format\n",
    "    for match in matches:\n",
    "        list_pattern = matcher.get(match[0])[1][0]\n",
    "        for token in list_pattern:\n",
    "            string_out += token['LOWER'] + ' '\n",
    "        string_out += ', '\n",
    "    string_out = string_out[:-3]\n",
    "    string_out = capitalize_string(string_out)\n",
    "    return string_out\n",
    "\n",
    "# Add a column consisting of the extracted streets\n",
    "df['streets'] = df['transcript'].map(location_extraction_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above Code Adapted from: Grant Wilson San Francisco Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Possible Street Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat list to house data from all addresses\n",
    "addresses = []\n",
    "\n",
    "# Loop Through all DataFrame's rows\n",
    "for row in df['transcript']:\n",
    "    # Create dictionary to house data for each row of the DataFrame\n",
    "    d = {}\n",
    "    \n",
    "    # Parse through rows and house results in a list\n",
    "    list_tuples = usaddress.parse(row)\n",
    "    \n",
    "    # Create variable to house list of possible numbers\n",
    "    numbers = []\n",
    "    \n",
    "    # Loop through each value in the list created\n",
    "    for i, n in enumerate(list_tuples):\n",
    "        \n",
    "        # Get addresses' numbers\n",
    "        if list_tuples[i][1] == 'AddressNumber':\n",
    "            \n",
    "            # Append numbers to list\n",
    "            numbers.append(n[0])\n",
    "    \n",
    "    # Include keys and values into d\n",
    "    d['numbers'] = numbers\n",
    "    \n",
    "    # Append d to addresses\n",
    "    addresses.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above Code Adapted from: Grant Wilson San Francisco Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Street Number Column to the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame(addresses)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a List of All Possible Adresses for each Row in the Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat list to house data for possible addresses\n",
    "possibilities = []\n",
    "\n",
    "# Loop Through all DataFrame's rows\n",
    "for i in range(0, df.shape[0]):\n",
    "    \n",
    "    # Create variables to temporarily house information\n",
    "    final_poss = []\n",
    "    d = {}\n",
    "    number_poss = []\n",
    "    \n",
    "    # Loop through values in each row / numbers\n",
    "    for row in df[i:i+1]['numbers']:\n",
    "        for a_number in row:\n",
    "            number_poss.append(a_number)\n",
    "    \n",
    "    # Loop through values in each row / streets\n",
    "    street_poss = []\n",
    "    for row2 in [x.split(',') for x in df[i:(i+1)]['streets']][0]:\n",
    "        for j in row2.split(','):\n",
    "            street_poss.append(j.strip())\n",
    "\n",
    "    # Concatenate numbers and streets\n",
    "    for i in number_poss:\n",
    "        for j in street_poss:\n",
    "            final_poss.append(i + ' ' + j)\n",
    "\n",
    "    # Append all possibilities to list\n",
    "    d['full_streets'] = list(set(final_poss))\n",
    "    possibilities.append(d)\n",
    "\n",
    "# Concatenate dataframes\n",
    "df = pd.concat([df, pd.DataFrame(possibilities)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above Code Adapted from: Grant Wilson San Francisco Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_streets'] = df['full_streets'].map(lambda x: np.nan if len(x) == 0 else x)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
